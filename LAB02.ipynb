{"cells":[{"cell_type":"markdown","metadata":{},"source":["Delta Lake로 데이터를 수집하도록 자동 로더 구성하기"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"820ec2c8-c3bb-4fd3-8e97-99dc1ae2d26c","showTitle":false,"title":""}},"outputs":[],"source":["# Import functions\n","from pyspark.sql.functions import input_file_name, current_timestamp\n","\n","# Define variables used in code below\n","file_path = \"/databricks-datasets/structured-streaming/events\"\n","username = spark.sql(\"SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')\").first()[0]\n","table_name = f\"{username}_etl_quickstart\"\n","checkpoint_path = f\"/tmp/{username}/_checkpoint/etl_quickstart\"\n","\n","# Clear out data from previous demo execution\n","spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","dbutils.fs.rm(checkpoint_path, True)\n","\n","# Configure Auto Loader to ingest JSON data to a Delta table\n","(spark.readStream\n","  .format(\"cloudFiles\")\n","  .option(\"cloudFiles.format\", \"json\")\n","  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n","  .load(file_path)\n","  .select(\"*\", input_file_name().alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\n","  .writeStream\n","  .option(\"checkpointLocation\", checkpoint_path)\n","  .trigger(availableNow=True)\n","  .toTable(table_name))"]},{"cell_type":"markdown","metadata":{},"source":["스토리지 계정으로 비구조적 데이터 수집하기"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"65122264-308b-439e-ac08-76ca87e31c4a","showTitle":false,"title":""}},"outputs":[],"source":["### Spark 통해서 컨테이너 탑재 ###\n","\n","#application id : Azure Active Directory > 앱 등록 > 개요 > 애플리케이션(클라이언트)ID\n","#secret : Azure Active Directory > 앱 등록 > 인증서 및 암호 > 클라이언트 암호의 [값]\n","#tenant id: Azure Active Directory > 앱 등록 > 개요 > 디렉터리(테넌트)ID\n","\n","\n","configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n","       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n","       \"fs.azure.account.oauth2.client.id\": \"<application-id>\", \n","       \"fs.azure.account.oauth2.client.secret\": \"<client-secret>\",\n","       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\",\n","       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n","\n","dbutils.fs.mount(\n","source = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/\",\n","mount_point = \"/mnt/flightdata\",\n","extra_configs = configs)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"a3c7438a-a3e2-4fea-977b-8ffeb3e25432","showTitle":false,"title":""}},"outputs":[],"source":["### Databricks Notebook 사용하여 CSV를 Parquet로 변환 ###\n","\n","# Use the previously established DBFS mount point to read the data.\n","# create a data frame to read data.\n","\n","flightDF = spark.read.format('csv').options(header='true', inferschema='true').load(\"/mnt/flightdata/*.csv\")\n","display(flightDF)\n","\n","# read the airline csv file and write the output to parquet format for easy query.\n","flightDF.write.mode(\"append\").parquet(\"/mnt/flightdata/parquet/flights\")\n","print(\"Done\")"]},{"cell_type":"markdown","metadata":{},"source":["데이터 살펴보기"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"8280fd71-fc5d-4e49-83cb-2661f98123eb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[],"type":"table"}},"output_type":"display_data"}],"source":["import os.path\n","import IPython\n","from pyspark.sql import SQLContext\n","display(dbutils.fs.ls(\"/mnt/flightdata\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"4232eba8-6971-4470-a30e-d2e2d82b4e2e","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.fs.put(\"/mnt/flightdata/1.txt\", \"Hello, World!\", True)\n","dbutils.fs.ls(\"/mnt/flightdata/parquet/flights\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"f513d843-1466-429e-b9e8-6a967d9e7e7c","showTitle":false,"title":""}},"outputs":[],"source":["# Copy this into a Cmd cell in your notebook.\n","acDF = spark.read.format('csv').options(\n","    header='true', inferschema='true').load(\"/mnt/flightdata/folder1/On_Time.csv/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_1\") #경로 확인!\n","acDF.write.parquet('/mnt/flightdata/parquet/airlinecodes')\n","\n","# read the existing parquet file for the flights database that was created earlier\n","flightDF = spark.read.format('parquet').options(\n","    header='true', inferschema='true').load(\"/mnt/flightdata/parquet/flights\")\n","\n","# print the schema of the dataframes\n","acDF.printSchema()\n","flightDF.printSchema()\n","\n","# print the flight database size\n","print(\"Number of flights in the database: \", flightDF.count())\n","\n","# show the first 20 rows (20 is the default)\n","# to show the first n rows, run: df.show(n)\n","acDF.show(100, False)\n","flightDF.show(20, False)\n","\n","# Display to run visualizations\n","# preferably run this in a separate cmd cell\n","display(flightDF)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d1810b39-df40-4146-925f-0ed3c4bfe879","showTitle":false,"title":""}},"outputs":[],"source":["# Run each of these queries, preferably in a separate cmd cell for separate analysis\n","# create a temporary sql view for querying flight information\n","FlightTable = spark.read.parquet('/mnt/flightdata/parquet/flights')\n","FlightTable.createOrReplaceTempView('FlightTable')\n","\n","# create a temporary sql view for querying airline code information\n","AirlineCodes = spark.read.parquet('/mnt/flightdata/parquet/airlinecodes')\n","AirlineCodes.createOrReplaceTempView('AirlineCodes')\n","\n","\n","# using spark sql, query the parquet file to return total flights in January and February 2016\n","out1 = spark.sql(\"SELECT * FROM AirlineCodes WHERE Month=1 AND Year = 2016 \")\n","NumJan2016Flights = out1.count()\n","out2 = spark.sql(\"SELECT * FROM AirlineCodes WHERE Month=2 AND Year = 2016 \")\n","NumFeb2016Flights = out2.count()\n","print(\"Jan 2016: \", NumJan2016Flights, \" Feb 2016: \", NumFeb2016Flights)\n","Total = NumJan2016Flights+NumFeb2016Flights\n","print(\"Total flights combined: \", Total)\n","\n","# List out all the airports in Texas\n","#out = spark.sql(\n","#    \"SELECT distinct(OriginCityName) FROM FlightTable where OriginStateName = 'Texas'\")\n","#print('Airports in Texas: ', out.show(100))\n","\n","# find all airlines that fly from Texas\n","#out1 = spark.sql(\n","#    \"SELECT distinct(Reporting_Airline) FROM FlightTable WHERE OriginStateName='Texas'\")\n","#print('Airlines that fly to/from Texas: ', out1.show(100, False))"]},{"cell_type":"markdown","metadata":{},"source":["DBFS(Databricks File System)에 업로드된 파일로 테이블 생성하기"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# File location and type\n","file_location = \"/FileStore/tables/nyc_taxi.csv\"\n","file_type = \"csv\"\n","\n","# CSV options\n","infer_schema = \"true\"\n","first_row_is_header = \"true\"\n","delimiter = \",\"\n","\n","# The applied options are for CSV files. For other file types, these will be ignored.\n","df = spark.read.format(file_type) \\\n","  .option(\"inferSchema\", infer_schema) \\\n","  .option(\"header\", first_row_is_header) \\\n","  .option(\"sep\", delimiter) \\\n","  .load(file_location)\n","\n","display(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a view or table (temp table로 생성되어 해당 Notebook에서만 사용가능)\n","\n","temp_table_name = \"nyc_taxi_csv\"\n","\n","df.createOrReplaceTempView(temp_table_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/*Query the created temp table in a SQL cell*/\n","\n","select * from `nyc_taxi_csv`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#With this registered as a temp view, it will only be available to this particular notebook. \n","#If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n","#Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n","#To do so, choose your table name and uncomment the bottom line.\n","#현 Cluster에 Permanent Table로 저장하기 \n","\n","permanent_table_name = \"nyc_taxi_csv\"\n","\n","df.write.format(\"parquet\").saveAsTable(permanent_table_name)"]},{"cell_type":"markdown","metadata":{},"source":["Delta Table 생성 및 데이터 삽입/수정/삭제, 테이블 삭제까지"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/*SET UP: 각 사용자로 범위가 지정된 USERNAME, USERHOME, DATABASE를 정의한다.*/\n","%run ../Includes/Classroom-Setup-2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* CREATE TABLE */\n","/* 관리되는 테이블\n","* Azure Databricks는 관리되는 테이블에 대한 [메타데이터], [데이터] 모두 관리함. 따라서 테이블 삭제 시, 기본 데이터도 삭제된다.\n","* 관리되는 테이블의 데이터는 등록된 데이터베이스의 LOCATION에 있다.\n","*/\n","\n","CREATE TABLE students\n","  (id INT, name STRING, value DOUBLE);\n","\n","/*\n","CREATE TABLE IF NOT EXISTS students \n","  (id INT, name STRING, value DOUBLE)\n","*/"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* CREATE TABLE */\n","/* 관리되지 않는 (외부) 테이블\n","* Azure Databricks는 관리되지 않는 (외부) 테이블에 대한 [메타데이터]만 관리함. 따라서 테이블 삭제 시, 기본 데이터에 영향 주지 않음.\n","* 관리되지 않는 테이블을 생성할 때는 항상 LOCATION을 지정한다.\n","*/\n","\n","create table delta_new(\n","\t\tnum int,\n","\t\tname string,\n","\t\tdate date\n","\t)using delta\n","LOCATION '[경로]'"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* INSERT */\n","\n","INSERT INTO delta_newtable VALUES(1, 'MADS', CURRENT_DATE());\n","\n","INSERT INTO students VALUES (1, \"Yve\", 1.0);\n","INSERT INTO students VALUES (2, \"Omar\", 2.5);\n","INSERT INTO students VALUES (3, \"Elia\", 3.3);\n","\n","/*\n","INSERT INTO students\n","VALUES \n","  (4, \"Ted\", 4.7),\n","  (5, \"Tiffany\", 5.5),\n","  (6, \"Vini\", 6.3)1) \n","*/"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* SELECT */\n","SELECT * FROM delta_newtable;\n","SELECT * FROM students;"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* UPDATE */\n","UPDATE students \n","SET value = value + 1\n","WHERE name LIKE \"T%\";"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* MERGE */\n","\n","CREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n","  (2, \"Omar\", 15.2, \"update\"),\n","  (3, \"\", null, \"delete\"),\n","  (7, \"Blue\", 7.7, \"insert\"),\n","  (11, \"Diya\", 8.8, \"update\");\n","  \n","SELECT * FROM updates;"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/*\n","- 데이터를 추가, 변경, 삭제하기 위해서 각각의 구문을 실행하는 방법도 있으나, 이럴 경우 3개의 개별 트랜잭션이 발생한다. 이 때 하나라도 실패하게 되면, 데이터가 잘못된 상태로 남아있을 수도 있다.\n","- 이 작업들을 결합하여 3가지 유형의 변경작업을 한번에 적용해보기로 한다.\n","- MERGE문에는 일치시킬 필드가 하나 이상 있어야 하며, 각 WHEN MATCHED / WHEN NOT MATCHED 절에는 조건문이 여러개 추가될 수 있다.\n","*/\n","MERGE INTO students b\n","USING updates u\n","ON b.id=u.id\n","WHEN MATCHED AND u.type = \"update\"\n","  THEN UPDATE SET *\n","WHEN MATCHED AND u.type = \"delete\"\n","  THEN DELETE\n","WHEN NOT MATCHED AND u.type = \"insert\"\n","  THEN INSERT *"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* DELETE */\n","\n","DELETE FROM students \n","WHERE value > 6 ;"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"sql"}},"outputs":[],"source":["/* DROP TABLE */\n","DROP TABLE students;\n","\n","/*현재까지 진행한 테이블과 파일들을 삭제\n","%python\n","DA.cleanup()\n","\n","*/"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1611610018873262,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"test","notebookOrigID":1945411588012978,"widgets":{}},"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.5"},"vscode":{"interpreter":{"hash":"1a47e62313ec3de59581e344c7436f71082b6d970008688e52c1cb8f51dccc8f"}}},"nbformat":4,"nbformat_minor":0}
